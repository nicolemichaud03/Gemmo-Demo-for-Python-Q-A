{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7809963,"sourceType":"datasetVersion","datasetId":4574236},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Demo of Google's Gemma Model - Answering Common Python Questions","metadata":{}},{"cell_type":"markdown","source":"By Nicole Michaud\n04/08/2024\n\n[Check out my Github here](http://github.com/nicolemichaud03)\n\n[Connect with me on LinkedIn](http://linkedin.com/in/nicole-michaud2)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n<img align=\"center\" src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdWSrhXoP4nHcjFcmbTNjhKcd3LRLcIiL2uEp4_8ilX7h_zsMu_muQlLP52eEgxvq4ejAQKy0TQKNaFC07O4o9imxqDDKF8hgaLU-iYfwmcPYGpm64psp1WHyaJOZQPImAhCDpYtc4nWEvbM3hSERTA50n08rIhftkP0rK1ai9uB-o3nWx0TQMRWt1leQ/w1200-h630-p-k-no-nu/Keras-Gemma-GfD.png\" width=\"700\">\n</div> \n","metadata":{}},{"cell_type":"markdown","source":"#### Purpose:\nThe goal of this notebook is to demonstrate how to use the Gemma LLM to answer common questions about the Python programming language. Gemma was trained on web documents, code, and mathematics, so these question and answer pairs about the Python programming language should work great with it! The data used for this task was sourced from FAQs on Python's website.","metadata":{}},{"cell_type":"markdown","source":"#### What is Gemma?\nGemma is a lightweight, open-source family of models from Google, built from the same research and technology used to create their Gemini models\nGemma models come in two different parameter size options: 2B and 7B. The 2B model is more lightweight and will perform just fine for this demo.","metadata":{}},{"cell_type":"markdown","source":"Let's get started!","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"markdown","source":"First, we need to import the necessary libraries for data processing and so that we are able to run this notebook on Kaggle with the Gemma model:","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-08T17:24:43.564020Z","iopub.execute_input":"2024-04-08T17:24:43.564473Z","iopub.status.idle":"2024-04-08T17:24:44.085057Z","shell.execute_reply.started":"2024-04-08T17:24:43.564424Z","shell.execute_reply":"2024-04-08T17:24:44.083840Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/gemma/keras/gemma_2b_en/2/config.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/tokenizer.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/metadata.json\n/kaggle/input/gemma/keras/gemma_2b_en/2/model.weights.h5\n/kaggle/input/gemma/keras/gemma_2b_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/python-faq-qa/python_faqs.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:24:44.086865Z","iopub.execute_input":"2024-04-08T17:24:44.087348Z","iopub.status.idle":"2024-04-08T17:25:22.351625Z","shell.execute_reply.started":"2024-04-08T17:24:44.087315Z","shell.execute_reply":"2024-04-08T17:25:22.350121Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:22.353938Z","iopub.execute_input":"2024-04-08T17:25:22.354436Z","iopub.status.idle":"2024-04-08T17:25:40.645622Z","shell.execute_reply.started":"2024-04-08T17:25:22.354384Z","shell.execute_reply":"2024-04-08T17:25:40.644213Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-08 17:25:24.721473: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-08 17:25:24.721678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-08 17:25:24.907211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Keras can be used to easily establish the basic architecture for the model to run on using either TensorFlow, JAX, or PyTorch. I am going to be using TensorFlow in this demo.","metadata":{}},{"cell_type":"code","source":"#establish a backend\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  \n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.648679Z","iopub.execute_input":"2024-04-08T17:25:40.649604Z","iopub.status.idle":"2024-04-08T17:25:40.654750Z","shell.execute_reply.started":"2024-04-08T17:25:40.649455Z","shell.execute_reply":"2024-04-08T17:25:40.653467Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The dataset that is going to be used was created from <a href=\"https://docs.python.org/3/faq/general.html\">Python's FAQs page</a> and includes the common questions, their answers, and what category they fall under.","metadata":{}},{"cell_type":"code","source":"#import the dataset\nimport pandas as pd\ndocuments = pd.read_csv(\"/kaggle/input/python-faq-qa/python_faqs.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.656847Z","iopub.execute_input":"2024-04-08T17:25:40.658236Z","iopub.status.idle":"2024-04-08T17:25:40.705672Z","shell.execute_reply.started":"2024-04-08T17:25:40.658173Z","shell.execute_reply":"2024-04-08T17:25:40.704464Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration","metadata":{}},{"cell_type":"code","source":"documents.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.708647Z","iopub.execute_input":"2024-04-08T17:25:40.709111Z","iopub.status.idle":"2024-04-08T17:25:40.729820Z","shell.execute_reply.started":"2024-04-08T17:25:40.709074Z","shell.execute_reply":"2024-04-08T17:25:40.728557Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                            Question  \\\n0  Why does Python use indentation for grouping o...   \n1  Why am I getting strange results with simple a...   \n2  Why are floating-point calculations so inaccur...   \n3                  Why are Python strings immutable?   \n4  Why must 'self' be used explicitly in method d...   \n\n                                              Answer Category  \n0  Guido van Rossum believes that using indentati...   design  \n1                             See the next question.   design  \n2  Users are often surprised by results like this...   design  \n3  \\nThere are several advantages.\\n\\nOne is perf...   design  \n4  The idea was borrowed from Modula-3. It turns ...   design  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Why does Python use indentation for grouping o...</td>\n      <td>Guido van Rossum believes that using indentati...</td>\n      <td>design</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Why am I getting strange results with simple a...</td>\n      <td>See the next question.</td>\n      <td>design</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Why are floating-point calculations so inaccur...</td>\n      <td>Users are often surprised by results like this...</td>\n      <td>design</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Why are Python strings immutable?</td>\n      <td>\\nThere are several advantages.\\n\\nOne is perf...</td>\n      <td>design</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Why must 'self' be used explicitly in method d...</td>\n      <td>The idea was borrowed from Modula-3. It turns ...</td>\n      <td>design</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"documents.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:54.335703Z","iopub.execute_input":"2024-04-08T17:25:54.336165Z","iopub.status.idle":"2024-04-08T17:25:54.362597Z","shell.execute_reply.started":"2024-04-08T17:25:54.336130Z","shell.execute_reply":"2024-04-08T17:25:54.361545Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 178 entries, 0 to 177\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Question  178 non-null    object\n 1   Answer    178 non-null    object\n 2   Category  178 non-null    object\ndtypes: object(3)\nmemory usage: 4.3+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataset contains 178 question-answer pairs.","metadata":{}},{"cell_type":"code","source":"documents['Category'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.731629Z","iopub.execute_input":"2024-04-08T17:25:40.732888Z","iopub.status.idle":"2024-04-08T17:25:40.745980Z","shell.execute_reply.started":"2024-04-08T17:25:40.732845Z","shell.execute_reply":"2024-04-08T17:25:40.744425Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array(['design', 'extending', 'general', 'gui', 'installed', 'library',\n       'programming', 'windows'], dtype=object)"},"metadata":{}}]},{"cell_type":"markdown","source":"The FAQ questions each fall into one of eight different categories.","metadata":{}},{"cell_type":"markdown","source":"## Modeling","metadata":{"execution":{"iopub.status.busy":"2024-03-10T19:42:13.341717Z","iopub.execute_input":"2024-03-10T19:42:13.342177Z","iopub.status.idle":"2024-03-10T19:42:13.347632Z","shell.execute_reply.started":"2024-03-10T19:42:13.342135Z","shell.execute_reply":"2024-03-10T19:42:13.346071Z"}}},{"cell_type":"markdown","source":"For this task I will be using the Gemma_2b model architecture, because I am running it on a personal computer that may not be able to support the 7b architecture. I will be using the GemmaCausalLM model, an end-to-end model for causal language modeling, configured with a preprocessor layer (GemmaCausalLMPreprocessor). By using a preprocessor, the string inputs are automatically preprocessed/tokenized, as opposed to having to do these steps prior to loading the model. The <a href=\"https://keras.io/api/keras_nlp/models/gemma/https://keras.io/api/keras_nlp/models/gemma/\" >Keras 3 API documentation</a> has more details on the different steps and options for Gemma's pretrained models.","metadata":{}},{"cell_type":"markdown","source":"#### Loading the Model:","metadata":{}},{"cell_type":"code","source":"# Load a preprocessor layer from a preset.\npreprocessor = keras_nlp.models.GemmaCausalLMPreprocessor.from_preset(\n    \"gemma_2b_en\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:28:51.048801Z","iopub.execute_input":"2024-04-08T17:28:51.050454Z","iopub.status.idle":"2024-04-08T17:28:53.102726Z","shell.execute_reply.started":"2024-04-08T17:28:51.050397Z","shell.execute_reply":"2024-04-08T17:28:53.101445Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\", preprocessor=preprocessor)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:28:57.113266Z","iopub.execute_input":"2024-04-08T17:28:57.114379Z","iopub.status.idle":"2024-04-08T17:30:53.423205Z","shell.execute_reply.started":"2024-04-08T17:28:57.114335Z","shell.execute_reply":"2024-04-08T17:30:53.421660Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Generating prompts:","metadata":{}},{"cell_type":"markdown","source":"Even before fine-tuning the model, we can use it to generate answers. First, we have to generate some sample prompts to be able to see how the model is doing at answering common Python questions, before we fine-tune it.First, we have to generate some sample prompts to be able to see how the model is doing at answering common Python questions, before we fine-tune it.","metadata":{}},{"cell_type":"code","source":"#Need to format each question-answer pair with a template that joins together and labels the Question, Answer and Category of each\n#to be able to be interpreted by the model\nfrom tqdm.notebook import tqdm\ntqdm.pandas() \ntemplate = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\ndocuments[\"prompt\"] = documents.progress_apply(lambda row: template.format(Category=row.Category,\n                                                             Question=row.Question,\n                                                             Answer=row.Answer), axis=1)\ndocuments = documents.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:31:40.013909Z","iopub.execute_input":"2024-04-08T17:31:40.014824Z","iopub.status.idle":"2024-04-08T17:31:40.070798Z","shell.execute_reply.started":"2024-04-08T17:31:40.014768Z","shell.execute_reply":"2024-04-08T17:31:40.069367Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/178 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d0fbc85e9414ac58eb78f8baa42c5f1"}},"metadata":{}}]},{"cell_type":"markdown","source":"For these prompts I selected 3 random questions from 3 different categories of FAQs, to get an idea of how the model performs on different types of questions.\n","metadata":{}},{"cell_type":"code","source":"# Question from \"general\" category\n\nprompt = template.format(\n    Question=\"What is Python?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:32:23.704943Z","iopub.execute_input":"2024-04-08T17:32:23.706235Z","iopub.status.idle":"2024-04-08T17:35:55.125140Z","shell.execute_reply.started":"2024-04-08T17:32:23.706186Z","shell.execute_reply":"2024-04-08T17:35:55.124118Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712597559.600229     490 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n2024-04-08 17:32:39.602030: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-08 17:32:39.602385: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n","output_type":"stream"},{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nWhat is Python?\n\nAnswer:\nPython is an interpreted, high-level, general-purpose programming language. It is\ndesigned to maximize programmer productivity with a combination of high-level\nsyntax, dynamic typing, dynamic binding, and high-level data structures, and a\nlow-level interpreted language, dynamic binding, and a low-level programming\nmodel. It is often used alongside other languages. Python is used to build web\napplications, data analysis tools, game engines, and more.\n\nCategory:\nkaggle-\n\nQuestion:\nWhat is R?\n\nAnswer:\nR is a programming language and software environment that supports statistical\ncomputation and graphical display. R was developed by Ross Ihaka and Robert\nSupportman in 1990, and it was originally called S-Plus. R is a programming language\nand software environment for statistical computing and graphics.\n\nCategory:\nkaggle-\n\nQuestion:\nWhat is RStudio?\n\nAnswer:\nRStudio is an integrated development environment (IDE) written in the programming\nlanguage R that was developed by a team of R developers, led by Jeffrey A.\nMontgomery (author of RStudio) at RStudio\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The specific question used in the prompt was given a pretty decent answer, but the other questions generated are not related to Python (they are about another programming language, R).","metadata":{}},{"cell_type":"code","source":"# Question from \"extending\" category\n\nprompt = template.format(\n    Question=\"How do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:35:55.127516Z","iopub.execute_input":"2024-04-08T17:35:55.128288Z","iopub.status.idle":"2024-04-08T17:38:15.324183Z","shell.execute_reply.started":"2024-04-08T17:35:55.128250Z","shell.execute_reply":"2024-04-08T17:38:15.323213Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nHow do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\n\nAnswer:\nThis can be done using the following code:\n\nimport sys\ndef my_print(*args, **kwargs):\n    if sys.stderr is not None:\n        sys.stderr.write(\"%s\\n\" % (\", \".join(map(str,args)),)\n    if sys.stdout is not None:\n        sys.stdout.write(\"%s\" % (\" \".join(map(str,args)),)\n    return\n\nmy_print(\"hello world\")\n\nOutput:\nhello world\n\n\nI hope this helps,\n\n-John\n\nI think you'll need to use ctypes to do it.\n\n-John\n\n\nYou need to catch the exception and then do a sys.stderr.flush() and sys.stdout.flush().\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The answer provided to this question is a bit confusing, it has text from someone named John, which seems a little out of place.","metadata":{}},{"cell_type":"code","source":"# Question from \"library\" category\n\nprompt = template.format(\n    Question=\"How do I program using threads?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:38:15.325790Z","iopub.execute_input":"2024-04-08T17:38:15.326155Z","iopub.status.idle":"2024-04-08T17:41:38.291690Z","shell.execute_reply.started":"2024-04-08T17:38:15.326124Z","shell.execute_reply":"2024-04-08T17:41:38.289882Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nHow do I program using threads?\n\nAnswer:\nThreads are not a programming language feature. They’re a feature in the operating system that allows one program to use another program’s code. You can use threads in Java and in C++ (using the C++ thread class).\n\nThe following code shows how to start the second thread.\n\n<code>Thread t1 = new Thread(new Runnable()\n{\n    @Override\n    public void run()\n    {\n        System.out.println(\"Hello from thread 1!\");\n    }\n});\n\nt1.start();\nt1.join(); // wait for thread to complete</code>\n\nIn the above example, the second thread starts and immediately exits (because the thread is not doing anything). The thread will not run until the first thread completes (because the second thread waits for the first thread to complete).\n\nThe following code shows how to use a thread to perform some work in the background and then return to the main thread.\n\n<code>Thread t = new Thread(new Runnable()\n{\n    public void run() \n    {\n        int i;\n        for(i = 0; i <=\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The above answers look okay, but some parts of them do not completely make sense for what is being asked. Let's see if we can improve them by fine-tuning the model.","metadata":{}},{"cell_type":"markdown","source":"### Fine-tuning with LoRA (Low-Rank Adaptation):","metadata":{}},{"cell_type":"markdown","source":"Gemma models and pre-trained, however, a process called fine-tuning can be used to further train them on a particular dataset!","metadata":{}},{"cell_type":"markdown","source":"According to the documentation, **LoRA** is a form of PEFT, or parameter-efficient fine-tuning. This is because it works by reducing down the number of trainable parameters and freezing model weights, causing the model to run faster and be more memory-efficient, while maintaining high-quality outputs! \nYou can learn more about the specifics of how LoRA works to fine-tune Gemma models <a href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=lSGRSsRPgkzKhttps://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=lSGRSsRPgkzK\">here!</a>\n","metadata":{}},{"cell_type":"markdown","source":"There are other options for tuning Gemma models, but for this demo, LoRA fine-tuning via KerasNLP is a good choice because it is known to be efficient and effective.","metadata":{}},{"cell_type":"markdown","source":"When enabling LoRA, the rank must be specified. The rank is related to the number of trainable parameters to be used, and a higher rank indicates that more detailed changes can be made to the model. However, the higher the rank, the more computationally expensive it will be. Therefore, it is best to start with a relatively low rank. Here, we use rank=4.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA freeze weights on the backbone, while allowing Lora to tune the query and value (question and answer) layers\n\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:42:50.111332Z","iopub.execute_input":"2024-04-08T17:42:50.112723Z","iopub.status.idle":"2024-04-08T17:42:50.314936Z","shell.execute_reply.started":"2024-04-08T17:42:50.112675Z","shell.execute_reply":"2024-04-08T17:42:50.313827Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,507,536,384\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"The total params are a little more than before enabling Lora, now 2,507,536,382 from 2,506,172,416. However, the trainable params are only 1,363,968 out of those whereas all of the params were trainable previously. This is because LoRa reduces trainable parameters in order to reduce the time and memory required for the model to run.","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 512\n#The Adam optimizer is used in the Gemma documentation, but other optimizers can be used as well\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(documents, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:42:52.098425Z","iopub.execute_input":"2024-04-08T17:42:52.098913Z","iopub.status.idle":"2024-04-08T21:08:50.065186Z","shell.execute_reply.started":"2024-04-08T17:42:52.098876Z","shell.execute_reply":"2024-04-08T21:08:50.062891Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[1m178/178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12357s\u001b[0m 69s/step - loss: 0.9697 - sparse_categorical_accuracy: 0.5263\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ca0ac5e57e0>"},"metadata":{}}]},{"cell_type":"markdown","source":"I used the same hyperparameter values as the documentation <a href = \"https://ai.google.dev/gemma/docs/lora_tuning#lora_fine-tuninghttps://ai.google.dev/gemma/docs/lora_tuning#lora_fine-tuning\">here</a>. Some other options for fine-tuning parameters include using a different optimizer, testing out different learning rates and weight decays, and changing the number of epochs and batch size. With respect to the loss, the model is adjusting its weights with each step. How fast or slow it does this adjustment is dictated by the **learning rate**. **Weight decay** is a regularization technique that will cause the weights to exponentially decay (via the specified learning rate) until they equal zero.","metadata":{}},{"cell_type":"markdown","source":"Generally, models perform better with multiple epochs, as the model is given more time to understand the trends within the data. Also, a larger batch size (the group of data points that the model is being fit on) can improve performance, but it can also cause overfitting (model is over-trained on the training data and therefore does not know how to respond to test/unseen data). ","metadata":{}},{"cell_type":"markdown","source":"As you can see, the above model took a long time to run. This runtime will only increase as you increase the number of epochs and the batch size. Therefore you must keep in mind the tradeoff between optimal model performance and runtime/memory usage.","metadata":{}},{"cell_type":"markdown","source":"### Inference after fine-tuning","metadata":{}},{"cell_type":"markdown","source":"Let's try out the same sample questions used before to see how the answers have changed after fine-tuning.","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    Question=\"What is Python?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:12:07.640639Z","iopub.execute_input":"2024-04-08T21:12:07.641337Z","iopub.status.idle":"2024-04-08T21:14:58.299112Z","shell.execute_reply.started":"2024-04-08T21:12:07.641292Z","shell.execute_reply":"2024-04-08T21:14:58.297709Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nWhat is Python?\n\nAnswer:\nPython is a general-purpose, high-level programming language that can be used for many different kinds of applications, including web development, scientific computing, and system scripting. It was created by Guido van Rossum in 1991, and is now maintained by The Python Software Foundation.\n\nPython is designed to be easy to read and write, yet powerful enough to be used by professionals. It is widely used for web development due to its built-in support for HTTP and other networking protocols, as well as its support for XML processing and database access. Python is also popular among scientific researchers, due to its support for numerical computing and data analysis.\n\nPython is widely available on most major operating systems, and is free and easy to install, making it a popular choice for beginners and professionals alike.\n\nIf you're looking for a general-purpose programming language that's easy to learn and use, Python is definitely worth considering.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    Question= \"How do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:14:58.302119Z","iopub.execute_input":"2024-04-08T21:14:58.302634Z","iopub.status.idle":"2024-04-08T21:17:06.618069Z","shell.execute_reply.started":"2024-04-08T21:14:58.302591Z","shell.execute_reply":"2024-04-08T21:17:06.617011Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nHow do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\n\nAnswer:\nThe answer is to redirect stdout and stderr to file, and then check the file for output.\n\n\nCategory:\nPython\n\nTags:\n\nArticle By:\nMichael A. Jackson\n\nArticle Last Updated:\n2014-05-10 08:40:00 -0500\n\nArticle Notes:\nhttp://www.pythonthegeek.com/2014/05/10/how-do-i-catch-the-output-from-pyerr_print-or-anything-that-prints-to-stdout-stderr/#comment-4447\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = template.format(\n    Question= \"How do I program using threads?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:17:06.619639Z","iopub.execute_input":"2024-04-08T21:17:06.620614Z","iopub.status.idle":"2024-04-08T21:17:47.148531Z","shell.execute_reply.started":"2024-04-08T21:17:06.620574Z","shell.execute_reply":"2024-04-08T21:17:47.147549Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n\nCategory:\nkaggle-\n\nQuestion:\nHow do I program using threads?\n\nAnswer:\nThe simplest way to do this is to use Python's threading library.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The generated answers for our sample questions are looking much better and making more sense after-fine tuning!","metadata":{}},{"cell_type":"markdown","source":"The next steps would be to continue tying out different hyperparameter values for fine-tuning to see how much we can improve the accuracy and the answers we get!","metadata":{}},{"cell_type":"markdown","source":"### Conclusion","metadata":{"execution":{"iopub.status.busy":"2024-03-10T19:42:44.012939Z","iopub.execute_input":"2024-03-10T19:42:44.013473Z","iopub.status.idle":"2024-03-10T19:42:44.018408Z","shell.execute_reply.started":"2024-03-10T19:42:44.013440Z","shell.execute_reply":"2024-03-10T19:42:44.017182Z"}}},{"cell_type":"markdown","source":"Google's new Gemma LLM models are lightweight and user friendly as they can be used with familiar APIs such as Keras. In this demo, I show you how to use Gemma with KerasNLP to answer common questions about the programming language Python! ","metadata":{}}]}