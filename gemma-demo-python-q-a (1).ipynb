{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7809963,"sourceType":"datasetVersion","datasetId":4574236},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Demo of Google's Gemma Model - Answering Common Python Questions","metadata":{}},{"cell_type":"markdown","source":"By Nicole Michaud\n04/08/2024\n\n[Check out my Github here](http://github.com/nicolemichaud03)\n\n[Connect with me on LinkedIn](http://linkedin.com/in/nicole-michaud2)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center;\">\n<img align=\"center\" src = \"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjdWSrhXoP4nHcjFcmbTNjhKcd3LRLcIiL2uEp4_8ilX7h_zsMu_muQlLP52eEgxvq4ejAQKy0TQKNaFC07O4o9imxqDDKF8hgaLU-iYfwmcPYGpm64psp1WHyaJOZQPImAhCDpYtc4nWEvbM3hSERTA50n08rIhftkP0rK1ai9uB-o3nWx0TQMRWt1leQ/w1200-h630-p-k-no-nu/Keras-Gemma-GfD.png\" width=\"700\">\n</div> \n","metadata":{}},{"cell_type":"markdown","source":"#### Purpose:\nThe goal of this notebook is to demonstrate how to use the Gemma LLM to answer common questions about the Python programming language. Gemma was trained on web documents, code, and mathematics, so these question and answer pairs about the Python programming language should work great with it! The data used for this task was sourced from FAQs on Python's website.","metadata":{}},{"cell_type":"markdown","source":"#### What is Gemma?\nGemma is a lightweight, open-source family of models from Google, built from the same research and technology used to create their Gemini models\nGemma models come in two different parameter size options: 2B and 7B. The 2B model is more lightweight and will perform just fine for this demo.","metadata":{}},{"cell_type":"markdown","source":"Let's get started!","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"markdown","source":"First, we need to import the necessary libraries for data processing and so that we are able to run this notebook on Kaggle with the Gemma model:","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-08T17:24:43.564020Z","iopub.execute_input":"2024-04-08T17:24:43.564473Z","iopub.status.idle":"2024-04-08T17:24:44.085057Z","shell.execute_reply.started":"2024-04-08T17:24:43.564424Z","shell.execute_reply":"2024-04-08T17:24:44.083840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:24:44.086865Z","iopub.execute_input":"2024-04-08T17:24:44.087348Z","iopub.status.idle":"2024-04-08T17:25:22.351625Z","shell.execute_reply.started":"2024-04-08T17:24:44.087315Z","shell.execute_reply":"2024-04-08T17:25:22.350121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:22.353938Z","iopub.execute_input":"2024-04-08T17:25:22.354436Z","iopub.status.idle":"2024-04-08T17:25:40.645622Z","shell.execute_reply.started":"2024-04-08T17:25:22.354384Z","shell.execute_reply":"2024-04-08T17:25:40.644213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keras can be used to easily establish the basic architecture for the model to run on using either TensorFlow, JAX, or PyTorch. I am going to be using TensorFlow in this demo.","metadata":{}},{"cell_type":"code","source":"#establish a backend\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  \n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.648679Z","iopub.execute_input":"2024-04-08T17:25:40.649604Z","iopub.status.idle":"2024-04-08T17:25:40.654750Z","shell.execute_reply.started":"2024-04-08T17:25:40.649455Z","shell.execute_reply":"2024-04-08T17:25:40.653467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset that is going to be used was created from <a href=\"https://docs.python.org/3/faq/general.html\">Python's FAQs page</a> and includes the common questions, their answers, and what category they fall under.","metadata":{}},{"cell_type":"code","source":"#import the dataset\nimport pandas as pd\ndocuments = pd.read_csv(\"/kaggle/input/python-faq-qa/python_faqs.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.656847Z","iopub.execute_input":"2024-04-08T17:25:40.658236Z","iopub.status.idle":"2024-04-08T17:25:40.705672Z","shell.execute_reply.started":"2024-04-08T17:25:40.658173Z","shell.execute_reply":"2024-04-08T17:25:40.704464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration","metadata":{}},{"cell_type":"code","source":"documents.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.708647Z","iopub.execute_input":"2024-04-08T17:25:40.709111Z","iopub.status.idle":"2024-04-08T17:25:40.729820Z","shell.execute_reply.started":"2024-04-08T17:25:40.709074Z","shell.execute_reply":"2024-04-08T17:25:40.728557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:54.335703Z","iopub.execute_input":"2024-04-08T17:25:54.336165Z","iopub.status.idle":"2024-04-08T17:25:54.362597Z","shell.execute_reply.started":"2024-04-08T17:25:54.336130Z","shell.execute_reply":"2024-04-08T17:25:54.361545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains 178 question-answer pairs.","metadata":{}},{"cell_type":"code","source":"documents['Category'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:25:40.731629Z","iopub.execute_input":"2024-04-08T17:25:40.732888Z","iopub.status.idle":"2024-04-08T17:25:40.745980Z","shell.execute_reply.started":"2024-04-08T17:25:40.732845Z","shell.execute_reply":"2024-04-08T17:25:40.744425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The FAQ questions each fall into one of eight different categories.","metadata":{}},{"cell_type":"markdown","source":"## Modeling","metadata":{"execution":{"iopub.status.busy":"2024-03-10T19:42:13.341717Z","iopub.execute_input":"2024-03-10T19:42:13.342177Z","iopub.status.idle":"2024-03-10T19:42:13.347632Z","shell.execute_reply.started":"2024-03-10T19:42:13.342135Z","shell.execute_reply":"2024-03-10T19:42:13.346071Z"}}},{"cell_type":"markdown","source":"For this task I will be using the Gemma_2b model architecture, because I am running it on a personal computer that may not be able to support the 7b architecture. I will be using the GemmaCausalLM model, an end-to-end model for causal language modeling, configured with a preprocessor layer (GemmaCausalLMPreprocessor). By using a preprocessor, the string inputs are automatically preprocessed/tokenized, as opposed to having to do these steps prior to loading the model. The <a href=\"https://keras.io/api/keras_nlp/models/gemma/https://keras.io/api/keras_nlp/models/gemma/\" >Keras 3 API documentation</a> has more details on the different steps and options for Gemma's pretrained models.","metadata":{}},{"cell_type":"markdown","source":"#### Loading the Model:","metadata":{}},{"cell_type":"code","source":"# Load a preprocessor layer from a preset.\npreprocessor = keras_nlp.models.GemmaCausalLMPreprocessor.from_preset(\n    \"gemma_2b_en\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:28:51.048801Z","iopub.execute_input":"2024-04-08T17:28:51.050454Z","iopub.status.idle":"2024-04-08T17:28:53.102726Z","shell.execute_reply.started":"2024-04-08T17:28:51.050397Z","shell.execute_reply":"2024-04-08T17:28:53.101445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\", preprocessor=preprocessor)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:28:57.113266Z","iopub.execute_input":"2024-04-08T17:28:57.114379Z","iopub.status.idle":"2024-04-08T17:30:53.423205Z","shell.execute_reply.started":"2024-04-08T17:28:57.114335Z","shell.execute_reply":"2024-04-08T17:30:53.421660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generating prompts:","metadata":{}},{"cell_type":"markdown","source":"Even before fine-tuning the model, we can use it to generate answers. First, we have to generate some sample prompts to be able to see how the model is doing at answering common Python questions, before we fine-tune it.First, we have to generate some sample prompts to be able to see how the model is doing at answering common Python questions, before we fine-tune it.","metadata":{}},{"cell_type":"code","source":"#Need to format each question-answer pair with a template that joins together and labels the Question, Answer and Category of each\n#to be able to be interpreted by the model\nfrom tqdm.notebook import tqdm\ntqdm.pandas() \ntemplate = \"\\n\\nCategory:\\nkaggle-{Category}\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\ndocuments[\"prompt\"] = documents.progress_apply(lambda row: template.format(Category=row.Category,\n                                                             Question=row.Question,\n                                                             Answer=row.Answer), axis=1)\ndocuments = documents.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:31:40.013909Z","iopub.execute_input":"2024-04-08T17:31:40.014824Z","iopub.status.idle":"2024-04-08T17:31:40.070798Z","shell.execute_reply.started":"2024-04-08T17:31:40.014768Z","shell.execute_reply":"2024-04-08T17:31:40.069367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For these prompts I selected 3 random questions from 3 different categories of FAQs, to get an idea of how the model performs on different types of questions.\n","metadata":{}},{"cell_type":"code","source":"# Question from \"general\" category\n\nprompt = template.format(\n    Question=\"What is Python?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:32:23.704943Z","iopub.execute_input":"2024-04-08T17:32:23.706235Z","iopub.status.idle":"2024-04-08T17:35:55.125140Z","shell.execute_reply.started":"2024-04-08T17:32:23.706186Z","shell.execute_reply":"2024-04-08T17:35:55.124118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The specific question used in the prompt was given a pretty decent answer, but the other questions generated are not related to Python (they are about another programming language, R).","metadata":{}},{"cell_type":"code","source":"# Question from \"extending\" category\n\nprompt = template.format(\n    Question=\"How do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:35:55.127516Z","iopub.execute_input":"2024-04-08T17:35:55.128288Z","iopub.status.idle":"2024-04-08T17:38:15.324183Z","shell.execute_reply.started":"2024-04-08T17:35:55.128250Z","shell.execute_reply":"2024-04-08T17:38:15.323213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The answer provided to this question is a bit confusing, it has text from someone named John, which seems a little out of place.","metadata":{}},{"cell_type":"code","source":"# Question from \"library\" category\n\nprompt = template.format(\n    Question=\"How do I program using threads?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:38:15.325790Z","iopub.execute_input":"2024-04-08T17:38:15.326155Z","iopub.status.idle":"2024-04-08T17:41:38.291690Z","shell.execute_reply.started":"2024-04-08T17:38:15.326124Z","shell.execute_reply":"2024-04-08T17:41:38.289882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above answers look okay, but some parts of them do not completely make sense for what is being asked. Let's see if we can improve them by fine-tuning the model.","metadata":{}},{"cell_type":"markdown","source":"### Fine-tuning with LoRA (Low-Rank Adaptation):","metadata":{}},{"cell_type":"markdown","source":"Gemma models and pre-trained, however, a process called fine-tuning can be used to further train them on a particular dataset!","metadata":{}},{"cell_type":"markdown","source":"According to the documentation, **LoRA** is a form of PEFT, or parameter-efficient fine-tuning. This is because it works by reducing down the number of trainable parameters and freezing model weights, causing the model to run faster and be more memory-efficient, while maintaining high-quality outputs! \nYou can learn more about the specifics of how LoRA works to fine-tune Gemma models <a href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=lSGRSsRPgkzKhttps://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=lSGRSsRPgkzK\">here!</a>\n","metadata":{}},{"cell_type":"markdown","source":"There are other options for tuning Gemma models, but for this demo, LoRA fine-tuning via KerasNLP is a good choice because it is known to be efficient and effective.","metadata":{}},{"cell_type":"markdown","source":"When enabling LoRA, the rank must be specified. The rank is related to the number of trainable parameters to be used, and a higher rank indicates that more detailed changes can be made to the model. However, the higher the rank, the more computationally expensive it will be. Therefore, it is best to start with a relatively low rank. Here, we use rank=4.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA freeze weights on the backbone, while allowing Lora to tune the query and value (question and answer) layers\n\ngemma_lm.backbone.enable_lora(rank=4)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:42:50.111332Z","iopub.execute_input":"2024-04-08T17:42:50.112723Z","iopub.status.idle":"2024-04-08T17:42:50.314936Z","shell.execute_reply.started":"2024-04-08T17:42:50.112675Z","shell.execute_reply":"2024-04-08T17:42:50.313827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The total params are a little more than before enabling Lora, now 2,507,536,382 from 2,506,172,416. However, the trainable params are only 1,363,968 out of those whereas all of the params were trainable previously. This is because LoRa reduces trainable parameters in order to reduce the time and memory required for the model to run.","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 512 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = 512\n#The Adam optimizer is used in the Gemma documentation, but other optimizers can be used as well\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\ngemma_lm.fit(documents, epochs=1, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T17:42:52.098425Z","iopub.execute_input":"2024-04-08T17:42:52.098913Z","iopub.status.idle":"2024-04-08T21:08:50.065186Z","shell.execute_reply.started":"2024-04-08T17:42:52.098876Z","shell.execute_reply":"2024-04-08T21:08:50.062891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the same hyperparameter values as the documentation <a href = \"https://ai.google.dev/gemma/docs/lora_tuning#lora_fine-tuninghttps://ai.google.dev/gemma/docs/lora_tuning#lora_fine-tuning\">here</a>. Some other options for fine-tuning parameters include using a different optimizer, testing out different learning rates and weight decays, and changing the number of epochs and batch size. With respect to the loss, the model is adjusting its weights with each step. How fast or slow it does this adjustment is dictated by the **learning rate**. **Weight decay** is a regularization technique that will cause the weights to exponentially decay (via the specified learning rate) until they equal zero.","metadata":{}},{"cell_type":"markdown","source":"Generally, models perform better with multiple epochs, as the model is given more time to understand the trends within the data. Also, a larger batch size (the group of data points that the model is being fit on) can improve performance, but it can also cause overfitting (model is over-trained on the training data and therefore does not know how to respond to test/unseen data). ","metadata":{}},{"cell_type":"markdown","source":"As you can see, the above model took a long time to run. This runtime will only increase as you increase the number of epochs and the batch size. Therefore you must keep in mind the tradeoff between optimal model performance and runtime/memory usage.","metadata":{}},{"cell_type":"markdown","source":"### Inference after fine-tuning","metadata":{}},{"cell_type":"markdown","source":"Let's try out the same sample questions used before to see how the answers have changed after fine-tuning.","metadata":{}},{"cell_type":"code","source":"prompt = template.format(\n    Question=\"What is Python?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:12:07.640639Z","iopub.execute_input":"2024-04-08T21:12:07.641337Z","iopub.status.idle":"2024-04-08T21:14:58.299112Z","shell.execute_reply.started":"2024-04-08T21:12:07.641292Z","shell.execute_reply":"2024-04-08T21:14:58.297709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = template.format(\n    Question= \"How do I catch the output from PyErr_Print() (or anything that prints to stdout/stderr)?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:14:58.302119Z","iopub.execute_input":"2024-04-08T21:14:58.302634Z","iopub.status.idle":"2024-04-08T21:17:06.618069Z","shell.execute_reply.started":"2024-04-08T21:14:58.302591Z","shell.execute_reply":"2024-04-08T21:17:06.617011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = template.format(\n    Question= \"How do I program using threads?\",\n    Answer=\"\",\n    Category=\"\"\n)\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"execution":{"iopub.status.busy":"2024-04-08T21:17:06.619639Z","iopub.execute_input":"2024-04-08T21:17:06.620614Z","iopub.status.idle":"2024-04-08T21:17:47.148531Z","shell.execute_reply.started":"2024-04-08T21:17:06.620574Z","shell.execute_reply":"2024-04-08T21:17:47.147549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The generated answers for our sample questions are looking much better and making more sense after-fine tuning!","metadata":{}},{"cell_type":"markdown","source":"The next steps would be to continue tying out different hyperparameter values for fine-tuning to see how much we can improve the accuracy and the answers we get!","metadata":{}},{"cell_type":"markdown","source":"### Conclusion","metadata":{"execution":{"iopub.status.busy":"2024-03-10T19:42:44.012939Z","iopub.execute_input":"2024-03-10T19:42:44.013473Z","iopub.status.idle":"2024-03-10T19:42:44.018408Z","shell.execute_reply.started":"2024-03-10T19:42:44.013440Z","shell.execute_reply":"2024-03-10T19:42:44.017182Z"}}},{"cell_type":"markdown","source":"Google's new Gemma LLM models are lightweight and user friendly as they can be used with familiar APIs such as Keras. In this demo, I show you how to use Gemma with KerasNLP to answer common questions about the programming language Python! ","metadata":{}}]}